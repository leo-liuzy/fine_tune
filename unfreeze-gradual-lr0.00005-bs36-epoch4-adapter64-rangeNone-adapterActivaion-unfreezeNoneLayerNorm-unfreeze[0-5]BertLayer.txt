To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
/home/leo/miniconda3/envs/py3/lib/python3.6/site-packages/IPython/paths.py:68: UserWarning: IPython parent '/gscratch/stf/zeyuliu2' is not a writable location, using a temp directory.
  " using a temp directory.".format(parent))
01/18/2020 14:51:38 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
01/18/2020 14:51:38 - INFO - modules.configuration_utils -   loading configuration file /home/leo/fine_tune/cache/bert-base-uncased-config.json
01/18/2020 14:51:38 - INFO - modules.configuration_utils -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

01/18/2020 14:51:38 - INFO - utils.tokenization_utils -   Model name '/home/leo/fine_tune/cache/bert-base-uncased-vocab.txt' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/home/leo/fine_tune/cache/bert-base-uncased-vocab.txt' is a path or url to a directory containing tokenizer files.
01/18/2020 14:51:38 - INFO - utils.tokenization_utils -   Didn't find file /home/leo/fine_tune/cache/added_tokens.json. We won't load it.
01/18/2020 14:51:38 - INFO - utils.tokenization_utils -   Didn't find file /home/leo/fine_tune/cache/special_tokens_map.json. We won't load it.
01/18/2020 14:51:38 - INFO - utils.tokenization_utils -   Didn't find file /home/leo/fine_tune/cache/tokenizer_config.json. We won't load it.
01/18/2020 14:51:38 - INFO - utils.tokenization_utils -   loading file /home/leo/fine_tune/cache/bert-base-uncased-vocab.txt
01/18/2020 14:51:38 - INFO - utils.tokenization_utils -   loading file None
01/18/2020 14:51:38 - INFO - utils.tokenization_utils -   loading file None
01/18/2020 14:51:38 - INFO - utils.tokenization_utils -   loading file None
01/18/2020 14:51:38 - INFO - modules.modeling_utils -   loading weights file /home/leo/fine_tune/cache/bert-base-uncased-pytorch_model.bin
01/18/2020 14:51:40 - INFO - modules.modeling_utils -   Weights of BertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']
01/18/2020 14:51:40 - INFO - modules.modeling_utils -   Weights from pretrained model not used in BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
01/18/2020 14:51:43 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, adapter_activation=1, adapter_range='None', apply_adapter_between_layer=False, apply_first_adapter_in_layer=False, apply_second_adapter_in_layer=False, bottleneck_size=64, cache_dir='', check=False, config_name='/home/leo/fine_tune/cache/bert-base-uncased-config.json', device=device(type='cuda', index=0), do_eval=True, do_lower_case=True, do_train=True, doc_stride=128, dropout=0.2, elmo_style=False, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gpu_id=0, gradient_accumulation_steps=3, init_scale=0.001, learning_rate=5e-05, local_rank=-1, logging_dir='/home/leo/fine_tune/logs', logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/home/leo/fine_tune/cache/bert-base-uncased-pytorch_model.bin', model_type='bert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_sample=-1, num_train_epochs=4, output_dir='/home/leo/fine_tune/out', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=12, predict_file='/home/leo/fine_tune/data/squad1_1/dev-v1.1.json', run_mode='single_run', save_steps=7386, seed=42, server_ip='', server_port='', tokenizer_name='/home/leo/fine_tune/cache/bert-base-uncased-vocab.txt', top_layer='linear', train_file='/home/leo/fine_tune/data/squad1_1/train-v1.1.json', unfreeze_attn_components='None', unfreeze_attn_dense_range='None', unfreeze_attn_range='None', unfreeze_bert_layer_range='[0-5]', unfreeze_embedding_components='None', unfreeze_intermediate_range='None', unfreeze_layernorm_range='None', unfreeze_output_dense_range='None', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
01/18/2020 14:51:43 - INFO - __main__ -   Loading features from cached file /home/leo/fine_tune/data/squad1_1/cached_train_bert-base-uncased-pytorch_model.bin_384
lr5e-05.epoch4.bs36.unfreeze_None_embedding.unfreeze_[0-5]_bert_layer.unfreeze_None_layer_None_attn.unfreeze_None_attn_dense.unfreeze_None_intermediate.unfreeze_None_output.unfreeze_None_layernorm
lr: 5e-05 	 num_train_epochs: 4
/home/leo/fine_tune/out
Namespace(adam_epsilon=1e-08, adapter_activation=1, adapter_range='None', apply_adapter_between_layer=False, apply_first_adapter_in_layer=False, apply_second_adapter_in_layer=False, bottleneck_size=64, cache_dir='', check=False, config_name='/home/leo/fine_tune/cache/bert-base-uncased-config.json', device=device(type='cuda', index=0), do_eval=True, do_lower_case=True, do_train=True, doc_stride=128, dropout=0.2, elmo_style=False, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gpu_id=0, gradient_accumulation_steps=3, init_scale=0.001, learning_rate=5e-05, local_rank=-1, logging_dir='/home/leo/fine_tune/logs', logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/home/leo/fine_tune/cache/bert-base-uncased-pytorch_model.bin', model_type='bert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_sample=-1, num_train_epochs=4, output_dir='/home/leo/fine_tune/out', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=12, predict_file='/home/leo/fine_tune/data/squad1_1/dev-v1.1.json', run_mode='single_run', save_steps=7386, seed=42, server_ip='', server_port='', tokenizer_name='/home/leo/fine_tune/cache/bert-base-uncased-vocab.txt', top_layer='linear', train_file='/home/leo/fine_tune/data/squad1_1/train-v1.1.json', unfreeze_attn_components='None', unfreeze_attn_dense_range='None', unfreeze_attn_range='None', unfreeze_bert_layer_range='[0-5]', unfreeze_embedding_components='None', unfreeze_intermediate_range='None', unfreeze_layernorm_range='None', unfreeze_output_dense_range='None', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
Parameters to be optimized: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'qa_outputs.weight', 'qa_outputs.bias']01/18/2020 14:51:55 - INFO - __main__ -   ***** Running training *****
01/18/2020 14:51:55 - INFO - __main__ -     Num examples = 88641
01/18/2020 14:51:55 - INFO - __main__ -     Num Epochs = 4
01/18/2020 14:51:55 - INFO - __main__ -     Instantaneous batch size per GPU = 12
01/18/2020 14:51:55 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 36
01/18/2020 14:51:55 - INFO - __main__ -     Gradient Accumulation steps = 3
01/18/2020 14:51:55 - INFO - __main__ -     Total optimization steps = 9848
Epoch:   0%|          | 0/4 [00:00<?, ?it/s]
Iteration:   0%|          | 0/7387 [00:00<?, ?it/s][A
> [0;32m/home/leo/fine_tune/runs/run_squad_unfreeze_gradual.py[0m(201)[0;36mtrain[0;34m()[0m
[0;32m    200 [0;31m            [0mbp[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 201 [0;31m            [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0;36m2[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    202 [0;31m                [0;32mif[0m [0mi[0m [0;34m==[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> --KeyboardInterrupt--
ipdb> ipdb> ipdb> ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> --KeyboardInterrupt--
ipdb> 
Exiting Debugger.
Epoch:   0%|          | 0/4 [07:36<?, ?it/s]
Iteration:   0%|          | 0/7387 [07:36<?, ?it/s]